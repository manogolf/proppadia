name: NHL Daily (Ingest → Refresh → Score → Export)

on:
  workflow_dispatch: {}
  schedule:
    - cron: "15 10 * * *" # ~03:15 PT daily (runs 10:15 UTC)

concurrency:
  group: nhl-daily
  cancel-in-progress: false

jobs:
  daily:
    runs-on: ubuntu-latest
    env:
      # REQUIRED repo secrets:
      # SUPABASE_DB_URL: postgres connection string (pooled is fine)
      # SUPABASE_URL:    https://<project>.supabase.co
      # SUPABASE_SERVICE_ROLE_KEY: service role key
      DATABASE_URL: ${{ secrets.SUPABASE_DB_URL }}
      SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      TZ: UTC

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set dates (UTC)
        run: |
          echo "DATE=$(date -u +%F)" >> $GITHUB_ENV
          echo "YDAY=$(date -u -d 'yesterday' +%F)" >> $GITHUB_ENV
          echo "DATE=${DATE} YDAY=${YDAY}"

      - name: Install psql client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Check DB connectivity
        run: psql "$SUPABASE_DB_URL" -v ON_ERROR_STOP=1 -c "SELECT now();"

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install Python deps (scripts only)
        run: |
          python -m pip install --upgrade pip
          # training/runtime libs your scripts might import
          if [ -f nhl/training/requirements.txt ]; then
            python -m pip install -r nhl/training/requirements.txt
          fi
          python -m pip install "psycopg[binary]>=3.1" "requests>=2.31"

      # ───────── MUST: import teams (only if you have a teams importer) ─────────
      # Optional: uncomment if you have these stage->merge steps scripted.
      # - name: Import/merge teams (optional)
      #   run: |
      #     python nhl/scripts/import_teams.py  # your script if present

      # ───────── MUST: import schedule into nhl.games ─────────
      - name: Import today's schedule (API → nhl.games)
        env:
          SLATE_DATE: ${{ env.DATE }}
        run: |
          python nhl/scripts/import_schedule_today.py

      # ───────── MUST: import rosters into nhl.roster_status ─────────
      - name: Import today's rosters (API → nhl.roster_status)
        env:
          SLATE_DATE: ${{ env.DATE }}
        run: |
          python nhl/scripts/import_roster_today.py

      # ───────── MUST: refresh features / views (if you have a SQL file) ─────────
      - name: Refresh features/views (if present)
        run: |
          set -e
          if [ -f nhl/scripts/refresh.sql ]; then
            echo "Running nhl/scripts/refresh.sql …"
            # Avoid timeout for heavier refreshes
            export PGOPTIONS='-c statement_timeout=0'
            psql "$SUPABASE_DB_URL" -v ON_ERROR_STOP=1 -f nhl/scripts/refresh.sql
          else
            echo "No refresh.sql found — skipping."
          fi

      # ───────── MUST: export slate feature CSVs for the scorer ─────────
      - name: Export slate CSVs (today only)
        run: |
          set -e
          export PGOPTIONS='-c statement_timeout=0'
          mkdir -p exports

          # Skater SOG
          # Use your dedicated view if you created it (preferred):
          if psql "$SUPABASE_DB_URL" -Atqc "SELECT 1 FROM pg_views WHERE schemaname='nhl' AND viewname='v_slate_sog_features'"; then
            psql "$SUPABASE_DB_URL" -v ON_ERROR_STOP=1 -c "\COPY (
              SELECT
                player_id, game_id, team_id, opponent_id, is_home, game_date,
                NULL::int AS shots_on_goal,
                d5_sog_per60, d10_sog_per60, d20_sog_per60,
                team_d10_sf_per_game, opp_d10_sf_allowed_per_game,
                role_pp_share, rest_days, b2b_flag, attempts_d10_per60,
                pace_index, opp_d10_sf_per60, team_d10_sa_per60, pace_matchup_index
              FROM nhl.v_slate_sog_features
              WHERE game_date = '${DATE}'::date
              ORDER BY game_id, player_id
            ) TO STDOUT WITH CSV HEADER" > exports/train_nhl_sog_v2.csv
          else
            # Fallback: if you didn’t create the view, export directly from training_features table
            psql "$SUPABASE_DB_URL" -v ON_ERROR_STOP=1 -c "\COPY (
              SELECT *
              FROM nhl.training_features_nhl_sog_v2
              WHERE game_date = '${DATE}'::date
              ORDER BY game_id, player_id
            ) TO STDOUT WITH CSV HEADER" > exports/train_nhl_sog_v2.csv
          fi

          # Goalie Saves
          if psql "$SUPABASE_DB_URL" -Atqc "SELECT 1 FROM pg_views WHERE schemaname='nhl' AND viewname='v_slate_saves_features'"; then
            psql "$SUPABASE_DB_URL" -v ON_ERROR_STOP=1 -c "\COPY (
              SELECT
                player_id, game_id, team_id, opponent_id, is_home, game_date,
                d10_shots_faced_per60, d10_save_pct,
                team_d10_sf_per_game, opp_d10_sf_allowed_per_game,
                pace_index, rest_days, b2b_flag,
                d5_saves_per60, d10_saves_per60, d5_shots_faced_per60, season_save_pct,
                opp_d10_sf_per60, team_d10_sa_per60, pace_matchup_index
              FROM nhl.v_slate_saves_features
              WHERE game_date = '${DATE}'::date
              ORDER BY game_id, player_id
            ) TO STDOUT WITH CSV HEADER" > exports/train_goalie_saves_v2.csv
          else
            psql "$SUPABASE_DB_URL" -v ON_ERROR_STOP=1 -c "\COPY (
              SELECT
                player_id, game_id, team_id, opponent_id, is_home, game_date,
                d10_shots_faced_per60, d10_save_pct,
                team_d10_sf_per_game, opp_d10_sf_allowed_per_game,
                pace_index, rest_days, b2b_flag,
                d5_saves_per60, d10_saves_per60, d5_shots_faced_per60, season_save_pct,
                opp_d10_sf_per60, team_d10_sa_per60, pace_matchup_index
              FROM nhl.training_features_goalie_saves_v2
              WHERE game_date = '${DATE}'::date
              ORDER BY game_id, player_id
            ) TO STDOUT WITH CSV HEADER" > exports/train_goalie_saves_v2.csv
          fi

          echo "Exported:"
          ls -lh exports/

      # ───────── MUST: score + load (your existing script) ─────────
      - name: Score & Load predictions
        run: |
          python nhl/scripts/run_daily_slate.py \
            --project nhl \
            --sog-csv exports/train_nhl_sog_v2.csv \
            --saves-csv exports/train_goalie_saves_v2.csv \
            --db-url "$SUPABASE_DB_URL"

      # Quick sanity counts
      - name: Verify counts
        run: |
          psql "$SUPABASE_DB_URL" -v ON_ERROR_STOP=1 -c "
            WITH g AS (SELECT game_id FROM nhl.games WHERE game_date = '${DATE}'::date)
            SELECT 'games_today' AS which, COUNT(*) FROM nhl.games WHERE game_date = '${DATE}'::date
            UNION ALL SELECT 'roster_rows_today', COUNT(*) FROM nhl.roster_status r WHERE r.game_id IN (SELECT game_id FROM g)
            UNION ALL SELECT 'sog_stage', COUNT(*) FROM nhl.predictions_sog_stage s WHERE s.game_id IN (SELECT game_id FROM g)
            UNION ALL SELECT 'saves_stage', COUNT(*) FROM nhl.predictions_saves_stage s WHERE s.game_id IN (SELECT game_id FROM g)
            UNION ALL SELECT 'predictions', COUNT(*) FROM nhl.predictions p WHERE p.game_id IN (SELECT game_id FROM g);
          "

      - name: Upload exported CSVs (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: nhl-exports-${{ env.DATE }}
          path: |
            exports/train_nhl_sog_v2.csv
            exports/train_goalie_saves_v2.csv
          if-no-files-found: warn
          retention-days: 14
